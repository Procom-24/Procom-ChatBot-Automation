{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Walkthough for Document Vectorization\n",
    "Throughout this walkthrough, we will be using OpenAI embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to import all the required libraries. <br> \n",
    "You can install them by executing a command in the terminal.<br><br>\n",
    "``pip install -r reqirements.txt``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13777/2754735617.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import openai\n",
    "# import langchain\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll introduce two functions for processing text from files.<br><br>\n",
    "<b>read_docx_as_single_page():</b> This function reads DOC files.<br>\n",
    "<b>read_pdf_as_single_page():</b> This function reads PDF files.<br><br>\n",
    "Each function takes the path of the file as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "\n",
    "def read_docx_as_single_page(docx_path: str) -> str:\n",
    "    doc = docx.Document(docx_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def read_pdf_as_single_page(pdf_path: str) -> str:\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        pdf_reader = PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            # Extract text from the current page and append it to the string\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "        return text\n",
    "    \n",
    "def read_csv_row_formatted(csv_path: str) -> str: #Added this function to help with .CSV file\n",
    "    test_df = pd.read_csv(csv_path, sep=',')\n",
    "    output = test_df.to_json(indent = 1, orient = 'records')\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code processes files and creates a list of documents. We will assume all files are in the <b>\"procom_collection\"</b> folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'procom_data'\n",
    "document = []\n",
    "metadata = []\n",
    "\n",
    "for file in os.listdir(folder_name):\n",
    "    # Get the file name for metadata.\n",
    "    file_name = file.split('.')[0]\n",
    "    metadata.append({\"competition\": file_name})\n",
    "    # If the file is a PDF then process it with \"read_pdf_as_single_page\".\n",
    "    # Elif the file is a PDF then process it with \"read_docx_as_single_page\".\n",
    "    # ELse process it with \"TextLoader\".\n",
    "    # Finally append the text string to the document list.\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = f\"./{folder_name}/\" + file\n",
    "        document.append(read_pdf_as_single_page(pdf_path))\n",
    "    elif file.endswith(\".docx\") or file.endswith(\".doc\"):\n",
    "        doc_path = f\"./{folder_name}/\" + file\n",
    "        document.append(read_docx_as_single_page(doc_path)) \n",
    "    elif file.endswith(\".txt\"):\n",
    "        txt_path = f\"./{folder_name}/\" + file\n",
    "        loader = TextLoader(txt_path)\n",
    "        document.append(loader.load()) \n",
    "    elif file.endswith(\".csv\"): #Added this function, reads CSV file in JSON format\n",
    "        csv_path = f\"./{folder_name}/\" + file\n",
    "        document.append(read_csv_row_formatted(csv_path))\n",
    "\n",
    "# print(document)\n",
    "# print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After retrieving the document, we will set up the Chroma database. Since the Chroma Database operates as a vector database, we must initially establish the embedding function.\n",
    "\n",
    "Chroma DB supports various embedding functions. We have the option to utilize either the default embedding function provided by Chroma DB or opt for the OpenAI embedding. For this project, we will utilize the OpenAI embedding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k224575/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 547kB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 225kB/s]\n",
      "README.md: 100%|██████████| 10.7k/10.7k [00:00<00:00, 18.4MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 58.3kB/s]\n",
      "config.json: 100%|██████████| 612/612 [00:00<00:00, 1.06MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:39<00:00, 2.30MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 726kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 590kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.28MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 548kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 168kB/s]\n"
     ]
    }
   ],
   "source": [
    "# To use the default embedding, you will\n",
    "# need to install \"sentence_transformers\" package \n",
    "# first.\n",
    " \n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting OpenAI variables\n",
    "OPEN_AI_API_KEY = \"OPENAI API KEY\"\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "openai_client = openai.OpenAI(api_key=OPEN_AI_API_KEY)\n",
    "\n",
    "# This is OpenAI embedding function\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=OPEN_AI_API_KEY,\n",
    "    model_name=embedding_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set the parameters of chroma db and add new data to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"Procom_Competitions\", embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "for i, text in enumerate(document):\n",
    "   collection.add(documents = document[i], metadatas=metadata[i], ids=[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing \n",
    "#collection.count()\n",
    "# t = collection.query(query_texts = \"events\")\n",
    "# t['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "\n",
    "# file = \"Procom'24 Plan Day1.csv\"\n",
    "# xlsx_path = f\"./{folder_name}/\" + file\n",
    "# s = pd.read_csv(xlsx_path, index_col = 0)\n",
    "# s = s.to_json(orient='records')\n",
    "# # formatted_data = []\n",
    "# # for row in s:\n",
    "# #     formatted_row = []\n",
    "# #     for key, value in row.items():\n",
    "# #         # If the value is a list, enclose it within square brackets\n",
    "# #         if isinstance(value, list):\n",
    "# #             value = \"[\" + \", \".join(map(str, value)) + \"]\"\n",
    "# #         formatted_row.append(value)\n",
    "# #     formatted_data.append(formatted_row)\n",
    "\n",
    "# # # Print or return the formatted data\n",
    "# # # for row in formatted_data:\n",
    "# # #     print(row)\n",
    "# # formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "# csv_file = \"procom_data/Procom'24 Plan Day1.csv\"\n",
    "# test_df = pd.read_csv(csv_file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n {\\n  \"Competitions\":\"Competitive Programming \",\\n  \"Capacity(Teams)\":70.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"Lab 1, Lab 2, Lab 4, Lab 5\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Lab 1 , Lab 2, Lab 4 \"\\n },\\n {\\n  \"Competitions\":\"Speed Debugging \",\\n  \"Capacity(Teams)\":35.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"E1, E2, E3 \",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Lab 10, Lab 12\"\\n },\\n {\\n  \"Competitions\":\"Chatcraft\",\\n  \"Capacity(Teams)\":25.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"Lab 8 \",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Lab 8\"\\n },\\n {\\n  \"Competitions\":\"database design\",\\n  \"Capacity(Teams)\":35.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"Lab 10, Lab 12\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Lab 5, Lab 6\"\\n },\\n {\\n  \"Competitions\":\"Code in the dark\",\\n  \"Capacity(Teams)\":35.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"Lab 5, Lab 8\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Lab 5, Lab 8\"\\n },\\n {\\n  \"Competitions\":\"Game developement \",\\n  \"Capacity(Teams)\":25.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"Outside Auditorium\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Outside Auditorium\"\\n },\\n {\\n  \"Competitions\":\"AI Showdown\",\\n  \"Capacity(Teams)\":45.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"D25, D26, D27, D28\",\\n  \"Lab\\\\/Room Allocated Day 2\":null\\n },\\n {\\n  \"Competitions\":\"Hackathon\",\\n  \"Capacity(Teams)\":35.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"C22, C21\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"C22, C21\"\\n },\\n {\\n  \"Competitions\":\"Code sprint\",\\n  \"Capacity(Teams)\":70.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"Lab 10, Lab 11, Lab 12, Lab 13\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Lab 11, Lab 12, Lab 13\"\\n },\\n {\\n  \"Competitions\":\"Web Developement\",\\n  \"Capacity(Teams)\":35.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"A4, A5, A6\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"A4, A5, A6\"\\n },\\n {\\n  \"Competitions\":\"Block chain\",\\n  \"Capacity(Teams)\":35.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"Lab 6,Lab 7\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Lab 7\"\\n },\\n {\\n  \"Competitions\":\"UI UX\",\\n  \"Capacity(Teams)\":35.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"Lab 11,Lab 13\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Lab 11, Lab 13\"\\n },\\n {\\n  \"Competitions\":\"App Developement \",\\n  \"Capacity(Teams)\":35.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"A1, A2, A3\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"A1, A2, A3\"\\n },\\n {\\n  \"Competitions\":\"Pseudowar\",\\n  \"Capacity(Teams)\":35.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"E4, E5, E6\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"E4, E5, E6\"\\n },\\n {\\n  \"Competitions\":\"Capture the Flag\",\\n  \"Capacity(Teams)\":45.0,\\n  \"Lab\\\\/Room Allocated Day 1 \":null,\\n  \"Lab\\\\/Room Allocated Day 2\":null\\n },\\n {\\n  \"Competitions\":\"Autocad\",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"Lab 11\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Lab 11\"\\n },\\n {\\n  \"Competitions\":\"Speed Soldering\",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"MPI Lab\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"MPI Lab\"\\n },\\n {\\n  \"Competitions\":\"Project Exibition\",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":null,\\n  \"Lab\\\\/Room Allocated Day 2\":null\\n },\\n {\\n  \"Competitions\":\"Robowar\",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":null,\\n  \"Lab\\\\/Room Allocated Day 2\":null\\n },\\n {\\n  \"Competitions\":\"Robo sumo\",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":null,\\n  \"Lab\\\\/Room Allocated Day 2\":null\\n },\\n {\\n  \"Competitions\":\"Robo soccer\",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":null,\\n  \"Lab\\\\/Room Allocated Day 2\":null\\n },\\n {\\n  \"Competitions\":\"Line following robot\",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":null,\\n  \"Lab\\\\/Room Allocated Day 2\":null\\n },\\n {\\n  \"Competitions\":\"FIFA \",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":null,\\n  \"Lab\\\\/Room Allocated Day 2\":null\\n },\\n {\\n  \"Competitions\":\"PUBG \",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":null,\\n  \"Lab\\\\/Room Allocated Day 2\":null\\n },\\n {\\n  \"Competitions\":\"TEKKEN \",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":null,\\n  \"Lab\\\\/Room Allocated Day 2\":null\\n },\\n {\\n  \"Competitions\":\"Chess\",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"D24\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"D24\"\\n },\\n {\\n  \"Competitions\":\"photography \",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"D22\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"D22\"\\n },\\n {\\n  \"Competitions\":\"reels \",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"D23\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"D23\"\\n },\\n {\\n  \"Competitions\":\"scavenger hunt\",\\n  \"Capacity(Teams)\":null,\\n  \"Lab\\\\/Room Allocated Day 1 \":\"Fourth Floor\",\\n  \"Lab\\\\/Room Allocated Day 2\":\"Fourth Floor\"\\n }\\n]'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING\n",
    "# json_output = \"procom_data/test.json\"\n",
    "# output = test_df.to_json(indent = 1, orient = 'records')\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> List[float]:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai_client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "def ask_procom_chatbot(user_query: str) -> str:\n",
    "    # Generate an embedded query from user query.\n",
    "    embedded_query = get_embedding(user_query, model=embedding_model)\n",
    "\n",
    "    # Retrieve all relevant data to teh user query\n",
    "    all_relevant_info = collection.query(\n",
    "        query_embeddings=embedded_query,\n",
    "        n_results=1,\n",
    "    )\n",
    "\n",
    "    # Separating and concatenating the retrieved data.\n",
    "    query_set = all_relevant_info[\"documents\"][0]\n",
    "    query_respond = \"\\n\".join(str(item) for item in query_set)\n",
    "\n",
    "    # Send the all info to the chatbot\n",
    "    chatbot_response = ask_openai(user_query, query_respond)\n",
    "    return chatbot_response\n",
    "\n",
    "\n",
    "def ask_openai(user_query: str, query_respond: str) -> str:\n",
    "    system_prompt = f\"\"\"\n",
    "        You are an informative chatbot specifically made for \n",
    "        an event called PROCOM. Your task is to answer user \n",
    "        questions based solely on the provided information. You can not answer the \n",
    "        user query from your knowledge. If the user's query is\n",
    "        within the scope of the provided information, you \n",
    "        will provide an answer. However, if the query is not \n",
    "        relevant to the information provided, you will politely inform the user\n",
    "        that it's out of your knowledge.\n",
    "        Below is the available information:\n",
    "        There are a total of 19 competitions listed below in Procom:\n",
    "        AI Showdown, App Dev, Blockchain Blitz, Chatcraft, Code In The Dark, \n",
    "        Code Sprint, Competitive Programming, CTF, Database Design, Game Dev,\n",
    "        Hackathon, LFR Rules, Psuedo War, ROBO SOCCER Rules, ROBO SUMO Rules, \n",
    "        ROBO WAR(Light Weight) Rules, Speed Debugging, UIUX, and Web Dev.\n",
    "        INFORMATION  \n",
    "        ####\n",
    "        {query_respond}\n",
    "        ####\n",
    "        Your response must be easy for user to understand.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=messages, temperature=0\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Game Dev competition, the rules include using only four allowed engines (Unity, Godot, Unreal, Pygame), bringing your own laptops, submitting a Github link for your engine, no exact replica games allowed, no inappropriate aesthetics, and addressing technical issues before the event starts. \n",
      "\n",
      "To make a team, you can have a maximum of 5 members. Each team needs to ensure creativity, follow the given theme (2D Roguelike Pixelated), and enhance a half-cooked game provided. Teams will be judged based on their modifications, creativity, and overall game quality in two rounds: The Creative Forge and The Arena of the Legend.\n"
     ]
    }
   ],
   "source": [
    "# user_input = \"How many events are there in Procom? can you name them?\"\n",
    "# user_input = \"Tell me about game dev?\"\n",
    "user_input = \"What are the rules of game dev? and how to make team?\"\n",
    "\n",
    "procom_chatbot_response = ask_procom_chatbot(user_input)\n",
    "print(procom_chatbot_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
